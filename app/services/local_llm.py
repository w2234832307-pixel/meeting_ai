"""
æœ¬åœ°LLMæœåŠ¡ï¼ˆæ”¯æŒQwen3-14bç­‰æœ¬åœ°éƒ¨ç½²æ¨¡å‹ï¼‰
ä½¿ç”¨OpenAIå…¼å®¹æ¥å£è°ƒç”¨æœ¬åœ°æ¨¡å‹
"""
import json
import re
from typing import Dict, Optional
from openai import OpenAI

from app.core.config import settings
from app.core.logger import logger
from app.core.exceptions import LLMServiceException


def remove_thinking_tags(text: str) -> str:
    """
    ç§»é™¤LLMè¾“å‡ºä¸­çš„æ€è€ƒè¿‡ç¨‹æ ‡ç­¾
    æ”¯æŒå¤šç§æ ¼å¼ï¼š
    1. <think>...</think>
    2. <p>...æ€è€ƒå†…å®¹...</p>...<h3>ä¼šè®®çºªè¦</h3>
    3. HTMLåµŒå¥—çš„å„ç§å˜ä½“
    """
    if not text:
        return text
    
    original_length = len(text)
    
    # === ç­–ç•¥1: ç§»é™¤æ ‡å‡† <think> æ ‡ç­¾ ===
    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # === ç­–ç•¥2: ç§»é™¤éæ ‡å‡†æ ¼å¼ - ä»å¼€å¤´åˆ°ç¬¬ä¸€ä¸ª Markdown æ ‡é¢˜ä¹‹å‰çš„æ‰€æœ‰å†…å®¹ ===
    # æ£€æµ‹æ˜¯å¦ä»¥ <p> å¼€å¤´ï¼Œä¸”åé¢æœ‰ Markdown æ ‡é¢˜ï¼ˆ###ã€##ã€#ï¼‰
    match = re.search(r'^[\s\S]*?(?=#{1,3}\s)', text)
    if match and match.group(0).strip().startswith('<p>'):
        # ç§»é™¤ä»å¼€å¤´åˆ°ç¬¬ä¸€ä¸ªæ ‡é¢˜ä¹‹å‰çš„æ‰€æœ‰ HTML æ®µè½ï¼ˆæ€è€ƒå†…å®¹ï¼‰
        text = re.sub(r'^.*?(?=#{1,3}\s)', '', text, flags=re.DOTALL)
        logger.info("ğŸ§¹ æ£€æµ‹åˆ°éæ ‡å‡†æ€è€ƒæ ¼å¼ï¼Œå·²ç§»é™¤å¼€å¤´çš„ HTML æ®µè½")
    
    # === ç­–ç•¥3: ç§»é™¤åŒ…å«æ€è€ƒå…³é”®è¯çš„ <p> æ®µè½ ===
    # å¸¸è§æ€è€ƒå…³é”®è¯ï¼šå¥½çš„ã€é¦–å…ˆã€æ¥ä¸‹æ¥ã€éœ€è¦æ³¨æ„ã€æœ€å
    thinking_keywords = [
        r'<p>[\s\S]*?å¥½çš„ï¼Œæˆ‘éœ€è¦.*?</p>',
        r'<p>[\s\S]*?é¦–å…ˆ.*?</p>',
        r'<p>[\s\S]*?æ¥ä¸‹æ¥.*?</p>',
        r'<p>[\s\S]*?éœ€è¦æ³¨æ„.*?</p>',
        r'<p>[\s\S]*?æœ€åï¼Œéœ€è¦ç¡®ä¿.*?</p>',
        r'<p>[\s\S]*?</think></p>',  # æ®‹ç•™çš„ </think> æ ‡ç­¾
    ]
    for pattern in thinking_keywords:
        text = re.sub(pattern, '', text, flags=re.DOTALL | re.IGNORECASE)
    
    # === æ¸…ç†æ®‹ç•™ ===
    # ç§»é™¤ç©ºçš„ <p> æ ‡ç­¾
    text = re.sub(r'<p>\s*</p>', '', text, flags=re.DOTALL)
    
    # ç§»é™¤å¼€å¤´çš„ <p> å’Œå¼•å·ï¼ˆå¦‚æœè¿˜æœ‰æ®‹ç•™ï¼‰
    text = re.sub(r'^[\s"]*<p>\s*', '', text)
    
    # ç§»é™¤å¤šä½™çš„ç©ºç™½è¡Œ
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    # å»é™¤å¼€å¤´å’Œç»“å°¾çš„ç©ºç™½å’Œå¼•å·
    text = text.strip().strip('"').strip()
    
    removed_chars = original_length - len(text)
    if removed_chars > 0:
        logger.info(f"ğŸ§¹ å·²æ¸…ç†æ€è€ƒå†…å®¹: ç§»é™¤ {removed_chars} å­—ç¬¦")
    
    return text


class LocalLLMService:
    """æœ¬åœ°LLMæœåŠ¡ç±»ï¼ˆQwen3-14bç­‰ï¼‰"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model_name: str = None, test_on_init: bool = None):
        """
        åˆå§‹åŒ–æœ¬åœ°LLMå®¢æˆ·ç«¯
        
        Args:
            api_key: APIå¯†é’¥ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
            base_url: APIåœ°å€ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
            model_name: æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
            test_on_init: æ˜¯å¦åˆå§‹åŒ–æ—¶æµ‹è¯•è¿æ¥ï¼ˆå¦‚æœä¸ºNoneï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ï¼‰
        """
        try:
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æœ¬åœ°LLMæœåŠ¡...")
            
            # ä½¿ç”¨ä¼ å…¥å‚æ•°æˆ–é…ç½®æ–‡ä»¶å€¼
            self.api_key = api_key or settings.LOCAL_LLM_API_KEY or "dummy-key"
            self.base_url = base_url or settings.LOCAL_LLM_BASE_URL
            self.model = model_name or settings.LOCAL_LLM_MODEL_NAME
            self.test_on_init = test_on_init if test_on_init is not None else settings.LOCAL_LLM_TEST_ON_INIT
            
            # åˆå§‹åŒ– OpenAI å…¼å®¹å®¢æˆ·ç«¯ï¼ˆæŒ‡å‘æœ¬åœ°æœåŠ¡ï¼‰
            self.client = OpenAI(
                api_key=self.api_key,
                base_url=self.base_url,
                timeout=settings.LLM_TIMEOUT
            )
            
            # æµ‹è¯•è¿æ¥
            if self.test_on_init:
                self._test_connection()
            
            logger.info(f"âœ… æœ¬åœ°LLMæœåŠ¡åˆå§‹åŒ–æˆåŠŸ (æ¨¡å‹: {self.model}, URL: {self.base_url})")
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°LLMæœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise LLMServiceException(f"æœ¬åœ°LLMåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    
    def _test_connection(self):
        """æµ‹è¯•æœ¬åœ°LLMæœåŠ¡è¿æ¥"""
        try:
            logger.info("ğŸ” æµ‹è¯•æœ¬åœ°LLMæœåŠ¡è¿æ¥...")
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "ä½ å¥½"}],
                max_tokens=10,
                temperature=0.1
            )
            if response and response.choices:
                logger.info("âœ… æœ¬åœ°LLMæœåŠ¡è¿æ¥æµ‹è¯•æˆåŠŸ")
            else:
                raise LLMServiceException("æœ¬åœ°LLMæœåŠ¡å“åº”å¼‚å¸¸")
        except Exception as e:
            logger.warning(f"âš ï¸ æœ¬åœ°LLMæœåŠ¡è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸åç»­ä½¿ç”¨æ—¶å†æŠ¥é”™
    
    def judge_rag(self, raw_text: str, template_id: str) -> dict:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦RAGæ£€ç´¢ï¼Œå¹¶æå–æœç´¢å…³é”®è¯
        
        Args:
            raw_text: åŸå§‹æ–‡æœ¬
            template_id: æ¨¡æ¿IDï¼ˆæš‚æœªä½¿ç”¨ï¼‰
        
        Returns:
            åŒ…å« need_rag å’Œ search_query çš„å­—å…¸
        """
        logger.info("ğŸ§  æœ¬åœ°LLM æ­£åœ¨åˆ†æ RAG æ„å›¾å¹¶æå–å…³é”®è¯...")
        
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼šè®®ç§˜ä¹¦ã€‚è¯·åˆ†æä»¥ä¸‹ä¼šè®®è®°å½•ï¼ˆASRè¯†åˆ«æ–‡æœ¬ï¼‰ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢å†å²çŸ¥è¯†åº“æ¥è¾…åŠ©ç”Ÿæˆçºªè¦ã€‚

ã€ä¼šè®®å†…å®¹ã€‘ï¼š
"{raw_text[:2000]}..." 

ã€åˆ¤æ–­æ ‡å‡†ã€‘ï¼š
å¦‚æœæ–‡ä¸­å‡ºç°äº†æ¨¡ç³ŠæŒ‡ä»£ï¼ˆå¦‚"ä¸Šæ¬¡è¯´çš„"ã€"é‚£ä¸ªé¡¹ç›®"ï¼‰æˆ–æåˆ°å…·ä½“çš„å†å²é—®é¢˜ã€æŠ€æœ¯åè¯ï¼Œåˆ™éœ€è¦æ£€ç´¢ã€‚

è¯·ä¸¥æ ¼è¿”å› JSON æ ¼å¼ï¼š
{{
    "need_rag": true,
    "search_query": "æå–å‡ºçš„æ ¸å¿ƒæœç´¢å…³é”®è¯ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œä¸è¦åŒ…å«åºŸè¯" 
}}
æˆ–è€…
{{
    "need_rag": false,
    "search_query": ""
}}
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                response_format={"type": "json_object"} if settings.LOCAL_LLM_SUPPORT_JSON_MODE else None
            )
            content = response.choices[0].message.content
            
            # è§£æJSONï¼ˆå³ä½¿ä¸æ”¯æŒjson_objectæ¨¡å¼ï¼Œä¹Ÿå°è¯•ä»æ–‡æœ¬ä¸­æå–ï¼‰
            result = self._extract_json_from_text(content)
            
            logger.info(f"âœ… RAGåˆ†æå®Œæˆ: need_rag={result.get('need_rag', False)}")
            return result
            
        except Exception as e:
            logger.error(f"âŒ RAG åˆ†æå¤±è´¥: {e}")
            return {"need_rag": False, "search_query": ""}
    
    def generate_markdown(self, raw_text: str, context: str = "", template_id: str = "default") -> str:
        """
        æ ¹æ®æ¨¡æ¿ç”Ÿæˆç»“æ„åŒ–æ•°æ®
        
        Args:
            raw_text: åŸå§‹æ–‡æœ¬
            context: RAGæ£€ç´¢åˆ°çš„å†å²ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
            template_id: æ¨¡æ¿IDï¼Œç”¨äºé€‰æ‹©ä¸åŒçš„ç”Ÿæˆæ¨¡æ¿
        
        Returns:
            æ ¹æ®æ¨¡æ¿ç”Ÿæˆçš„ç»“æ„åŒ–æ•°æ®ï¼ˆMarkdownæ ¼å¼ï¼‰
        """
        logger.info(f"ğŸ§  æœ¬åœ°LLM æ­£åœ¨æ ¹æ®æ¨¡æ¿ '{template_id}' ç”Ÿæˆç»“æ„åŒ–æ•°æ®...")
        
        # æ ¹æ®æ¨¡æ¿IDé€‰æ‹©ä¸åŒçš„æ¨¡æ¿
        template = self._get_template(template_id)
        
        # ç»„è£… Prompt
        system_prompt = template.get("system_prompt", "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é«˜çº§ç§˜ä¹¦ï¼Œè´Ÿè´£å°†è¯­éŸ³è¯†åˆ«çš„æ–‡æœ¬æ•´ç†æˆç»“æ„æ¸…æ™°çš„æ•°æ®ã€‚")
        
        user_input = template.get("user_prompt_template", "").format(
            context=context if context else "æ— ",
            raw_text=raw_text
        )
        
        if not user_input:
            # é»˜è®¤æ¨¡æ¿
            user_input = f"""
è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆä¼šè®®çºªè¦ã€‚

ã€å‚è€ƒå†å²ä¿¡æ¯ã€‘ï¼š
{context if context else "æ— "}

ã€åŸå§‹è¯­éŸ³æ–‡æœ¬ã€‘ï¼š
{raw_text}

ã€è¦æ±‚ã€‘ï¼š
1. ä½¿ç”¨ Markdown æ ¼å¼ã€‚
2. åŒ…å«æ ‡é¢˜ã€å‚ä¸äººã€å†³ç­–ç»“è®ºã€å¾…åŠäº‹é¡¹ã€‚
3. å»é™¤å£è¯­åºŸè¯ã€‚
4. ç»“æ„åŒ–è¾“å‡ºï¼Œä¾¿äºåç»­å¤„ç†ã€‚
"""

        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input}
                ],
                temperature=0.3,
                max_tokens=settings.LOCAL_LLM_MAX_TOKENS
            )
            content = response.choices[0].message.content
            usage = response.usage
            tokens = (usage.total_tokens if usage else 0)
            logger.info(f"âœ… ç”Ÿæˆå®Œæˆï¼Œæ¶ˆè€— Token: {tokens}")
            return content
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            raise LLMServiceException(f"æœ¬åœ°LLMç”Ÿæˆå¤±è´¥: {str(e)}")
    
    def _get_template(self, template_id: str) -> Dict[str, str]:
        """
        è·å–æ¨¡æ¿é…ç½®
        
        Args:
            template_id: æ¨¡æ¿ID
        
        Returns:
            æ¨¡æ¿é…ç½®å­—å…¸ï¼ŒåŒ…å« system_prompt å’Œ user_prompt_template
        """
        templates = {
            "default": {
                "system_prompt": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é«˜çº§ç§˜ä¹¦ï¼Œè´Ÿè´£å°†è¯­éŸ³è¯†åˆ«çš„æ–‡æœ¬æ•´ç†æˆç»“æ„æ¸…æ™°çš„ Markdown ä¼šè®®çºªè¦ã€‚",
                "user_prompt_template": """
è¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”Ÿæˆä¼šè®®çºªè¦ã€‚

ã€å‚è€ƒå†å²ä¿¡æ¯ã€‘ï¼š
{context}

ã€åŸå§‹è¯­éŸ³æ–‡æœ¬ã€‘ï¼š
{raw_text}

ã€è¦æ±‚ã€‘ï¼š
1. ä½¿ç”¨ Markdown æ ¼å¼ã€‚
2. åŒ…å«æ ‡é¢˜ã€å‚ä¸äººã€å†³ç­–ç»“è®ºã€å¾…åŠäº‹é¡¹ã€‚
3. å»é™¤å£è¯­åºŸè¯ã€‚
4. ç»“æ„åŒ–è¾“å‡ºï¼Œä¾¿äºåç»­å¤„ç†ã€‚
"""
            },
        }
        
        return templates.get(template_id, templates["default"])
    
    def chat(self, prompt: str, temperature: float = 0.7, max_tokens: int = 2000) -> str:
        """
        ç®€å•çš„èŠå¤©æ¥å£ï¼ˆç”¨äºæ–°çš„åŠ¨æ€æ¨¡æ¿ç³»ç»Ÿï¼‰
        
        Args:
            prompt: å®Œæ•´çš„æç¤ºè¯
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
        
        Returns:
            æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬
        """
        logger.info("ğŸ’¬ æœ¬åœ°LLM Chat è°ƒç”¨...")
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            content = response.choices[0].message.content
            # æ¸…ç†æ€è€ƒè¿‡ç¨‹
            content = remove_thinking_tags(content)
            logger.info(f"âœ… æœ¬åœ°LLM ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(content)}")
            return content
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°LLM Chat è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    def _extract_json_from_text(self, text: str) -> dict:
        """
        ä»æ–‡æœ¬ä¸­æå–JSONï¼ˆå…¼å®¹ä¸æ”¯æŒjson_objectæ¨¡å¼çš„æ¨¡å‹ï¼‰
        
        Args:
            text: åŒ…å«JSONçš„æ–‡æœ¬
        
        Returns:
            è§£æåçš„å­—å…¸
        """
        try:
            # å°è¯•ç›´æ¥è§£æ
            return json.loads(text)
        except json.JSONDecodeError:
            # å°è¯•æŸ¥æ‰¾JSONä»£ç å—
            import re
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(1))
            
            # å°è¯•æŸ¥æ‰¾è£¸JSON
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group(0))
            
            # è§£æå¤±è´¥ï¼Œè¿”å›é»˜è®¤å€¼
            logger.warning(f"âš ï¸ æ— æ³•ä»æ–‡æœ¬ä¸­æå–JSON: {text[:200]}")
            return {"need_rag": False, "search_query": ""}


# åˆ›å»ºå•ä¾‹å®ä¾‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
_local_llm_service_instance = None

def get_local_llm_service():
    """è·å–æœ¬åœ°LLMæœåŠ¡å•ä¾‹"""
    global _local_llm_service_instance
    if _local_llm_service_instance is None:
        _local_llm_service_instance = LocalLLMService()
    return _local_llm_service_instance
