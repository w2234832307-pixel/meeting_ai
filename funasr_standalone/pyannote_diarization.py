#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Pyannote è¯´è¯äººåˆ†ç¦»æ¨¡å—
ä½¿ç”¨ä¸“ä¸šçš„ Pyannote.audio æ¨¡å‹è¿›è¡Œè¯´è¯äººåˆ†ç¦»
"""
import logging
from typing import List, Dict, Optional
from pathlib import Path
import os
import shutil
import tempfile

import torch
import soundfile as sf
import subprocess
import tempfile

logger = logging.getLogger(__name__)

try:
    from pyannote.audio import Pipeline
    PYANNOTE_AVAILABLE = True
except ImportError:
    PYANNOTE_AVAILABLE = False
    logger.warning("âš ï¸ Pyannote.audio æœªå®‰è£…ï¼Œè¯´è¯äººåˆ†ç¦»åŠŸèƒ½å°†ä¸å¯ç”¨")
    logger.warning("   å®‰è£…å‘½ä»¤: pip install pyannote.audio")


# å…¨å±€ pipeline ç¼“å­˜ï¼ˆé¿å…é‡å¤åŠ è½½ï¼‰
_pipeline_cache = None


def _extract_annotation(diarization):
    """
    å…¼å®¹ä¸åŒç‰ˆæœ¬ Pyannoteï¼š
    - æ—§ç‰ˆ: pipeline(...) ç›´æ¥è¿”å›æ”¯æŒ itertracks çš„ Annotation
    - 3.x: å¯èƒ½è¿”å› DiarizeOutputï¼ŒçœŸæ­£çš„ Annotation åœ¨æŸä¸ªå±æ€§é‡Œ
    """
    # æ—§ç‰ˆï¼šç›´æ¥å°±æ˜¯ Annotation
    if hasattr(diarization, "itertracks"):
        return diarization

    # å°è¯•é€šè¿‡ __dict__ æˆ– vars() è®¿é—®ï¼ˆé€‚ç”¨äº dataclass/NamedTupleï¼‰
    try:
        obj_dict = vars(diarization) if hasattr(diarization, "__dict__") else {}
        for key, value in obj_dict.items():
            if value is not None and hasattr(value, "itertracks"):
                return value
    except:
        pass

    # å°è¯•é€šè¿‡ç´¢å¼•è®¿é—®ï¼ˆå¦‚æœæ˜¯ NamedTupleï¼‰
    try:
        if hasattr(diarization, "__len__"):
            for i in range(len(diarization)):
                ann = diarization[i]
                if ann is not None and hasattr(ann, "itertracks"):
                    return ann
    except:
        pass

    # æ–°ç‰ˆï¼šDiarizeOutput dataclass - å°è¯•å¤šç§å¯èƒ½çš„å±æ€§å
    possible_attrs = ["annotation", "speaker", "output", "result", "diarization", "labels"]
    ann = None
    
    for attr in possible_attrs:
        try:
            ann = getattr(diarization, attr, None)
            if ann is not None and hasattr(ann, "itertracks"):
                return ann
        except:
            continue
    
    # å¦‚æœæ˜¯ dataclassï¼Œå°è¯•è®¿é—®æ‰€æœ‰å­—æ®µ
    if hasattr(diarization, "__dataclass_fields__"):
        for field_name in diarization.__dataclass_fields__.keys():
            try:
                ann = getattr(diarization, field_name, None)
                if ann is not None and hasattr(ann, "itertracks"):
                    return ann
            except:
                continue
    
    # å°è¯• dir() æŸ¥æ‰¾æ‰€æœ‰å±æ€§
    for attr_name in dir(diarization):
        if attr_name.startswith("_"):
            continue
        try:
            ann = getattr(diarization, attr_name)
            if ann is not None and hasattr(ann, "itertracks"):
                return ann
        except:
            continue

    # æœ‰äº›ç‰ˆæœ¬å¯èƒ½è¿”å› dict
    if isinstance(diarization, dict):
        for key in ["annotation", "speaker", "output", "result", "diarization"]:
            ann = diarization.get(key)
            if ann is not None and hasattr(ann, "itertracks"):
                return ann

    # å¦‚æœè¿˜æ˜¯æ‰¾ä¸åˆ°ï¼Œæ‰“å°æ‰€æœ‰å±æ€§ä»¥ä¾¿è°ƒè¯•
    attrs = [a for a in dir(diarization) if not a.startswith("_")]
    try:
        obj_dict = vars(diarization) if hasattr(diarization, "__dict__") else {}
        logger.error(f"DiarizeOutput å¯¹è±¡è¯¦æƒ…: {obj_dict}")
    except:
        pass
    
    raise TypeError(
        f"Unsupported diarization output type: {type(diarization)}\n"
        f"Available attributes: {attrs}\n"
        f"Type: {type(diarization).__name__}\n"
        f"è¯·æ£€æŸ¥ Pyannote ç‰ˆæœ¬å’Œ DiarizeOutput çš„å®é™…ç»“æ„"
    )


def detect_long_silence_with_vad(waveform: torch.Tensor, sample_rate: int, min_silence_duration: float = 2.0) -> List[tuple]:
    """
    ä½¿ç”¨è½»é‡çº§ VAD æ£€æµ‹é•¿é™éŸ³ç‚¹ï¼ˆ>2ç§’ï¼‰- ä¼˜åŒ–ç‰ˆæœ¬
    
    Args:
        waveform: éŸ³é¢‘æ³¢å½¢ tensorï¼Œå½¢çŠ¶ä¸º [channels, time]
        sample_rate: é‡‡æ ·ç‡
        min_silence_duration: æœ€å°é™éŸ³æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 2.0 ç§’
    
    Returns:
        é™éŸ³æ®µåˆ—è¡¨ [(start_time, end_time), ...]ï¼Œå•ä½ï¼šç§’
    """
    logger.info(f"ğŸ” æ£€æµ‹é•¿é™éŸ³ç‚¹ï¼ˆ>={min_silence_duration}ç§’ï¼‰...")
    
    try:
        # ä¼˜åŒ–ï¼šä¸‹é‡‡æ ·ä»¥åŠ å¿«å¤„ç†é€Ÿåº¦å¹¶é™ä½å†…å­˜å ç”¨
        # å¯¹äºé™éŸ³æ£€æµ‹ï¼Œä¸éœ€è¦é«˜ç²¾åº¦ï¼Œå¯ä»¥å¤§å¹…é™ä½é‡‡æ ·ç‡
        downsample_factor = 8  # é™ä½åˆ° 1/8 é‡‡æ ·ç‡ï¼ˆè¿›ä¸€æ­¥é™ä½å†…å­˜å ç”¨ï¼‰
        if downsample_factor > 1:
            # ç®€å•ä¸‹é‡‡æ ·ï¼šæ¯éš” N ä¸ªç‚¹å–ä¸€ä¸ª
            if waveform.ndim > 1:
                downsampled = waveform[:, ::downsample_factor]
            else:
                downsampled = waveform[::downsample_factor]
            effective_sample_rate = sample_rate / downsample_factor
        else:
            downsampled = waveform
            effective_sample_rate = sample_rate
        
        # è®¡ç®—éŸ³é¢‘èƒ½é‡ï¼ˆRMSï¼‰- ä½¿ç”¨æ‰¹é‡è®¡ç®—
        if downsampled.ndim > 1:
            audio_energy = torch.sqrt(torch.mean(downsampled ** 2, dim=0))
        else:
            audio_energy = torch.abs(downsampled)
        
        # å½’ä¸€åŒ–
        max_energy = torch.max(audio_energy)
        if max_energy > 0:
            audio_energy = audio_energy / max_energy
        
        # ä½¿ç”¨æ›´å¤§çš„å¸§çª—å£ä»¥åŠ å¿«å¤„ç†å¹¶é™ä½å†…å­˜å ç”¨ï¼ˆ2ç§’ä¸€å¸§ï¼‰
        frame_duration = 2.0  # æ”¹ä¸º 2 ç§’ä¸€å¸§ï¼Œè¿›ä¸€æ­¥é™ä½å†…å­˜å ç”¨
        frame_samples = int(effective_sample_rate * frame_duration)
        
        # æ‰¹é‡è®¡ç®—æ¯å¸§çš„å¹³å‡èƒ½é‡ï¼ˆä½¿ç”¨æ›´å°çš„æ‰¹æ¬¡ï¼‰
        num_frames = len(audio_energy) // frame_samples
        if num_frames > 0:
            # åˆ†æ‰¹å¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰å¸§åˆ°å†…å­˜
            batch_size = 1000  # æ¯æ‰¹å¤„ç† 1000 å¸§
            frame_energies_list = []
            for i in range(0, num_frames, batch_size):
                end_idx = min(i + batch_size, num_frames)
                batch_frames = audio_energy[i * frame_samples:end_idx * frame_samples]
                if len(batch_frames) >= frame_samples:
                    batch_frames = batch_frames[:len(batch_frames) // frame_samples * frame_samples]
                    frames = batch_frames.view(-1, frame_samples)
                    frame_energies_list.append(torch.mean(frames, dim=1))
            if frame_energies_list:
                frame_energies = torch.cat(frame_energies_list)
            else:
                frame_energies = torch.tensor([torch.mean(audio_energy)])
        else:
            frame_energies = torch.tensor([torch.mean(audio_energy)])
        
        # æ£€æµ‹é™éŸ³ï¼ˆèƒ½é‡ä½äºé˜ˆå€¼ï¼‰- æé«˜é˜ˆå€¼ï¼Œå‡å°‘è¯¯æ£€
        energy_threshold = 0.05  # æé«˜é˜ˆå€¼åˆ° 0.05
        
        # æ‰¾åˆ°è¿ç»­é™éŸ³æ®µï¼ˆä½¿ç”¨æ‰¹é‡æ“ä½œï¼‰
        silence_frames = frame_energies < energy_threshold
        
        # ä½¿ç”¨ torch çš„æ‰¹é‡æ“ä½œæ‰¾åˆ°é™éŸ³æ®µ
        silence_segments = []
        in_silence = False
        silence_start = 0
        
        for i in range(len(silence_frames)):
            if silence_frames[i] and not in_silence:
                in_silence = True
                silence_start = i * frame_duration
            elif not silence_frames[i] and in_silence:
                in_silence = False
                silence_end = i * frame_duration
                if silence_end - silence_start >= min_silence_duration:
                    silence_segments.append((silence_start, silence_end))
        
        # å¤„ç†æœ€åä¸€ä¸ªé™éŸ³æ®µ
        if in_silence:
            silence_end = len(silence_frames) * frame_duration
            if silence_end - silence_start >= min_silence_duration:
                silence_segments.append((silence_start, silence_end))
        
        logger.info(f"âœ… æ£€æµ‹åˆ° {len(silence_segments)} ä¸ªé•¿é™éŸ³æ®µï¼ˆ>={min_silence_duration}ç§’ï¼‰")
        return silence_segments
        
    except Exception as e:
        logger.warning(f"âš ï¸ VAD æ£€æµ‹å¤±è´¥: {e}")
        return []


def split_audio_by_silence(waveform: torch.Tensor, sample_rate: int, 
                           silence_segments: List[tuple],
                           min_chunk_duration: float = 600.0,  # 10 åˆ†é’Ÿ
                           max_chunk_duration: float = 1200.0) -> List[tuple]:
    """
    æ ¹æ®é™éŸ³ç‚¹å°†éŸ³é¢‘åˆ‡åˆ†ä¸º 10-20 åˆ†é’Ÿçš„ç‰‡æ®µï¼ˆä¼˜åŒ–ç‰ˆï¼‰
    
    Args:
        waveform: éŸ³é¢‘æ³¢å½¢ tensor
        sample_rate: é‡‡æ ·ç‡
        silence_segments: é™éŸ³æ®µåˆ—è¡¨ [(start, end), ...]
        min_chunk_duration: æœ€å°ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 600 ç§’ï¼ˆ10 åˆ†é’Ÿï¼‰
        max_chunk_duration: æœ€å¤§ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 1200 ç§’ï¼ˆ20 åˆ†é’Ÿï¼‰
    
    Returns:
        éŸ³é¢‘ç‰‡æ®µåˆ—è¡¨ [(start_time, end_time), ...]ï¼Œå•ä½ï¼šç§’
    """
    audio_duration = waveform.shape[-1] / sample_rate
    chunks = []
    
    if not silence_segments or len(silence_segments) > 1000:
        # é™éŸ³æ®µå¤ªå¤šæˆ–æ²¡æœ‰ï¼Œç›´æ¥æŒ‰æœ€å¤§æ—¶é•¿åˆ‡åˆ†ï¼ˆé¿å…è¿‡åº¦åˆ‡åˆ†ï¼‰
        logger.info(f"âš ï¸ é™éŸ³æ®µæ•°é‡å¼‚å¸¸ï¼ˆ{len(silence_segments) if silence_segments else 0}ï¼‰ï¼Œä½¿ç”¨å›ºå®šæ—¶é•¿åˆ‡åˆ†")
        num_chunks = int(audio_duration / max_chunk_duration) + 1
        for i in range(num_chunks):
            start = i * max_chunk_duration
            end = min((i + 1) * max_chunk_duration, audio_duration)
            chunks.append((start, end))
        logger.info(f"âœ‚ï¸ åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µï¼ˆæ¯æ®µ {max_chunk_duration}ç§’ï¼‰")
        return chunks
    
    # ä¼˜åŒ–ï¼šåªé€‰æ‹©è¶³å¤Ÿé•¿çš„é™éŸ³æ®µï¼ˆ>3ç§’ï¼‰ä½œä¸ºåˆ‡åˆ†ç‚¹
    valid_silence_points = []
    for silence_start, silence_end in silence_segments:
        silence_duration = silence_end - silence_start
        if silence_duration >= 3.0:  # åªä½¿ç”¨ >3ç§’ çš„é™éŸ³æ®µ
            # ä½¿ç”¨é™éŸ³æ®µçš„ä¸­é—´ç‚¹ä½œä¸ºåˆ‡åˆ†ç‚¹
            cut_point = (silence_start + silence_end) / 2
            valid_silence_points.append(cut_point)
    
    # æ ¹æ®æœ‰æ•ˆé™éŸ³ç‚¹åˆ‡åˆ†
    current_start = 0.0
    
    for cut_point in valid_silence_points:
        chunk_duration = cut_point - current_start
        
        # å¦‚æœè¾¾åˆ°æœ€å°æ—¶é•¿ï¼Œåœ¨åˆ‡åˆ†ç‚¹åˆ‡åˆ†
        if chunk_duration >= min_chunk_duration:
            chunks.append((current_start, cut_point))
            current_start = cut_point
        
        # å¦‚æœè¶…è¿‡æœ€å¤§æ—¶é•¿ï¼Œå¼ºåˆ¶åˆ‡åˆ†
        if chunk_duration >= max_chunk_duration:
            chunks.append((current_start, cut_point))
            current_start = cut_point
    
    # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
    if audio_duration - current_start > 0:
        chunks.append((current_start, audio_duration))
    
    # å¦‚æœåˆ‡åˆ†ç»“æœå¤ªå°‘ï¼Œå›é€€åˆ°å›ºå®šåˆ‡åˆ†
    if len(chunks) == 0 or (len(chunks) == 1 and chunks[0][1] - chunks[0][0] > max_chunk_duration * 2):
        logger.info("âš ï¸ åˆ‡åˆ†ç»“æœä¸ç†æƒ³ï¼Œå›é€€åˆ°å›ºå®šæ—¶é•¿åˆ‡åˆ†")
        chunks = []
        num_chunks = int(audio_duration / max_chunk_duration) + 1
        for i in range(num_chunks):
            start = i * max_chunk_duration
            end = min((i + 1) * max_chunk_duration, audio_duration)
            chunks.append((start, end))
    
    logger.info(f"âœ‚ï¸ æ ¹æ®é™éŸ³ç‚¹åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªç‰‡æ®µï¼ˆ10-20åˆ†é’Ÿï¼‰")
    return chunks


def extract_speaker_embeddings(pipeline, waveform: torch.Tensor, sample_rate: int, 
                               annotation) -> Dict[str, torch.Tensor]:
    """
    ä» diarization ç»“æœä¸­æå–æ¯ä¸ª speaker çš„ embedding
    
    Args:
        pipeline: Pyannote pipeline å¯¹è±¡
        waveform: éŸ³é¢‘æ³¢å½¢ tensor
        sample_rate: é‡‡æ ·ç‡
        annotation: Pyannote Annotation å¯¹è±¡
    
    Returns:
        {speaker_id: embedding_tensor, ...}
    """
    speaker_embeddings = {}
    
    try:
        # è·å– embedding æ¨¡å‹
        # Pyannote pipeline çš„ embedding å¯èƒ½é€šè¿‡å¤šç§æ–¹å¼è®¿é—®
        embedding_model = None
        
        # æ–¹æ³•1ï¼šç›´æ¥è®¿é—® pipeline.embedding
        if hasattr(pipeline, "embedding"):
            embedding_attr = pipeline.embedding
            if isinstance(embedding_attr, str):
                try:
                    from pyannote.audio import Model
                    embedding_model = Model.from_pretrained(embedding_attr)
                    if torch.cuda.is_available():
                        embedding_model = embedding_model.to(torch.device("cuda"))
                    logger.info(f"âœ… ä»è·¯å¾„åŠ è½½ embedding æ¨¡å‹: {embedding_attr}")
                except Exception as e:
                    logger.debug(f"âš ï¸ æ— æ³•ä»è·¯å¾„åŠ è½½ embedding æ¨¡å‹: {e}")
            elif hasattr(embedding_attr, "__call__") or hasattr(embedding_attr, "forward"):
                embedding_model = embedding_attr
                logger.info("âœ… ä½¿ç”¨ pipeline.embedding æ¨¡å‹å¯¹è±¡")
        
        # æ–¹æ³•2ï¼šè®¿é—® pipeline._embedding
        if embedding_model is None and hasattr(pipeline, "_embedding"):
            embedding_attr = pipeline._embedding
            if isinstance(embedding_attr, str):
                try:
                    from pyannote.audio import Model
                    embedding_model = Model.from_pretrained(embedding_attr)
                    if torch.cuda.is_available():
                        embedding_model = embedding_model.to(torch.device("cuda"))
                    logger.info(f"âœ… ä»è·¯å¾„åŠ è½½ embedding æ¨¡å‹: {embedding_attr}")
                except Exception as e:
                    logger.debug(f"âš ï¸ æ— æ³•ä»è·¯å¾„åŠ è½½ embedding æ¨¡å‹: {e}")
            elif hasattr(embedding_attr, "__call__") or hasattr(embedding_attr, "forward"):
                embedding_model = embedding_attr
                logger.info("âœ… ä½¿ç”¨ pipeline._embedding æ¨¡å‹å¯¹è±¡")
        
        # æ–¹æ³•3ï¼šé€šè¿‡ pipeline çš„å†…éƒ¨ç»“æ„è®¿é—®ï¼ˆå°è¯•å¤šç§å¯èƒ½çš„å±æ€§åï¼‰
        if embedding_model is None:
            possible_attrs = ["embedding_model", "_embedding_model", "speaker_embedding", "_speaker_embedding"]
            for attr_name in possible_attrs:
                if hasattr(pipeline, attr_name):
                    attr_value = getattr(pipeline, attr_name)
                    if hasattr(attr_value, "__call__") or hasattr(attr_value, "forward"):
                        embedding_model = attr_value
                        logger.info(f"âœ… é€šè¿‡ {attr_name} è·å– embedding æ¨¡å‹")
                        break
        
        # æ–¹æ³•4ï¼šä» pipeline çš„ embedding ç»„ä»¶è·å–ï¼ˆå¦‚æœ pipeline æœ‰ embedding ç»„ä»¶ï¼‰
        if embedding_model is None and hasattr(pipeline, "components"):
            components = pipeline.components
            if isinstance(components, dict) and "embedding" in components:
                embedding_comp = components["embedding"]
                if hasattr(embedding_comp, "model"):
                    embedding_model = embedding_comp.model
                    logger.info("âœ… é€šè¿‡ pipeline.components['embedding'].model è·å– embedding æ¨¡å‹")
                elif hasattr(embedding_comp, "__call__") or hasattr(embedding_comp, "forward"):
                    embedding_model = embedding_comp
                    logger.info("âœ… é€šè¿‡ pipeline.components['embedding'] è·å– embedding æ¨¡å‹")
        
        if embedding_model is None:
            logger.warning("âš ï¸ æ— æ³•è·å– embedding æ¨¡å‹ï¼Œè·³è¿‡å£°çº¹æå–ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰")
            logger.debug(f"âš ï¸ Pipeline å±æ€§: {[attr for attr in dir(pipeline) if 'embed' in attr.lower()]}")
            return speaker_embeddings
        
        # å¯¹æ¯ä¸ª speaker segment æå– embedding
        from pyannote.core import Segment
        
        for turn, _, speaker in annotation.itertracks(yield_label=True):
            # ç¡®ä¿ speaker ä¸ä¸º None
            if speaker is None or speaker == "":
                speaker = "SPEAKER_UNKNOWN"
            
            # æå–è¯¥ segment çš„éŸ³é¢‘
            start_sample = int(turn.start * sample_rate)
            end_sample = int(turn.end * sample_rate)
            segment_waveform = waveform[:, start_sample:end_sample]
            
            # ç¡®ä¿ segment_waveform åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if hasattr(embedding_model, "device"):
                model_device = embedding_model.device
            elif hasattr(embedding_model, "_device"):
                model_device = embedding_model._device
            else:
                model_device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
            if segment_waveform.device != model_device:
                segment_waveform = segment_waveform.to(model_device)
            
            # æå– embedding
            with torch.no_grad():
                try:
                    # å°è¯•ä¸åŒçš„è°ƒç”¨æ–¹å¼
                    embedding = None
                    
                    # æ–¹å¼1ï¼šç›´æ¥è°ƒç”¨ï¼ˆ__call__ï¼‰
                    if hasattr(embedding_model, "__call__"):
                        try:
                            embedding = embedding_model({"waveform": segment_waveform, "sample_rate": sample_rate})
                        except Exception as e1:
                            logger.debug(f"âš ï¸ __call__ æ–¹å¼å¤±è´¥: {e1}")
                    
                    # æ–¹å¼2ï¼šforward æ–¹æ³•
                    if embedding is None and hasattr(embedding_model, "forward"):
                        try:
                            embedding = embedding_model.forward({"waveform": segment_waveform, "sample_rate": sample_rate})
                        except Exception as e2:
                            logger.debug(f"âš ï¸ forward æ–¹å¼å¤±è´¥: {e2}")
                    
                    # æ–¹å¼3ï¼šä½¿ç”¨ pipeline çš„ embedding æ–¹æ³•ï¼ˆå¦‚æœæœ‰ï¼‰
                    if embedding is None and hasattr(pipeline, "embedding") and callable(pipeline.embedding):
                        try:
                            embedding = pipeline.embedding({"waveform": segment_waveform, "sample_rate": sample_rate})
                        except Exception as e3:
                            logger.debug(f"âš ï¸ pipeline.embedding æ–¹å¼å¤±è´¥: {e3}")
                    
                    if embedding is None:
                        continue
                    
                    # embedding å¯èƒ½æ˜¯ tensor æˆ– dictï¼Œéœ€è¦å¤„ç†
                    if isinstance(embedding, dict):
                        embedding = embedding.get("embedding", embedding.get("output", embedding.get("logits", embedding.get("embeddings", None))))
                    if embedding is None or not isinstance(embedding, torch.Tensor):
                        continue
                except Exception as e:
                    logger.debug(f"âš ï¸ æå– segment embedding å¤±è´¥: {e}")
                    continue
                
                # å¦‚æœæ˜¯å¤šå¸§ï¼Œå–å¹³å‡
                if embedding.ndim > 1:
                    embedding = torch.mean(embedding, dim=0)
                
                # ç´¯ç§¯è¯¥ speaker çš„ embeddingï¼ˆå–å¹³å‡ï¼‰
                if speaker not in speaker_embeddings:
                    speaker_embeddings[speaker] = []
                speaker_embeddings[speaker].append(embedding)
        
        # å¯¹æ¯ä¸ª speaker çš„å¤šä¸ª embedding å–å¹³å‡
        for speaker in speaker_embeddings:
            embeddings_list = speaker_embeddings[speaker]
            if len(embeddings_list) > 1:
                speaker_embeddings[speaker] = torch.mean(torch.stack(embeddings_list), dim=0)
            else:
                speaker_embeddings[speaker] = embeddings_list[0]
        
        logger.info(f"âœ… æå–äº† {len(speaker_embeddings)} ä¸ª speaker çš„ embedding")
        
    except Exception as e:
        logger.warning(f"âš ï¸ æå– speaker embedding å¤±è´¥: {e}")
    
    return speaker_embeddings


def global_speaker_calibration(all_chunk_results: List[Dict], 
                               global_embeddings: Dict[str, torch.Tensor],
                               threshold: float = 0.7) -> Dict[str, str]:
    """
    å…¨å±€å£°çº¹æ ¡å‡†ï¼šå¯¹è·¨ç‰‡æ®µçš„ speaker ID è¿›è¡Œèšç±»æ ¡å‡†
    
    Args:
        all_chunk_results: æ‰€æœ‰ç‰‡æ®µçš„ diarization ç»“æœ
            [{"chunk_idx": 0, "annotation": Annotation, "embeddings": {...}}, ...]
        global_embeddings: å…¨å±€ speaker embedding å­—å…¸
            {"chunk_0_SPEAKER_00": embedding, "chunk_1_SPEAKER_01": embedding, ...}
        threshold: èšç±»é˜ˆå€¼ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    
    Returns:
        speaker ID æ˜ å°„å­—å…¸ {"chunk_0_SPEAKER_00": "SPEAKER_00", ...}
    """
    try:
        try:
            from sklearn.cluster import AgglomerativeClustering
            from sklearn.metrics.pairwise import cosine_similarity
        except ImportError:
            logger.warning("âš ï¸ sklearn æœªå®‰è£…ï¼Œæ— æ³•è¿›è¡Œå…¨å±€æ ¡å‡†ï¼Œè¯·å®‰è£…: pip install scikit-learn")
            return {}
        import numpy as np
        
        if not global_embeddings:
            logger.warning("âš ï¸ æ²¡æœ‰ embeddingï¼Œè·³è¿‡å…¨å±€æ ¡å‡†")
            return {}
        
        # å‡†å¤‡æ•°æ®
        speaker_ids = list(global_embeddings.keys())
        embeddings_matrix = torch.stack([global_embeddings[sid] for sid in speaker_ids]).cpu().numpy()
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = cosine_similarity(embeddings_matrix)
        
        # ä½¿ç”¨å±‚æ¬¡èšç±»
        # è·ç¦» = 1 - ç›¸ä¼¼åº¦
        distance_matrix = 1 - similarity_matrix
        
        # èšç±»
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold=1 - threshold,  # è·ç¦»é˜ˆå€¼
            linkage='average',
            metric='precomputed'
        )
        cluster_labels = clustering.fit_predict(distance_matrix)
        
        # åˆ›å»ºæ˜ å°„ï¼šæ¯ä¸ª speaker_id -> å…¨å±€ç»Ÿä¸€çš„ speaker_id
        # ä½¿ç”¨æ¯ä¸ª cluster ä¸­ç¬¬ä¸€ä¸ª speaker çš„ ID ä½œä¸ºä»£è¡¨
        id_mapping = {}
        cluster_to_global_id = {}
        global_id_counter = 0
        
        for i, speaker_id in enumerate(speaker_ids):
            cluster_id = cluster_labels[i]
            
            if cluster_id not in cluster_to_global_id:
                # åˆ›å»ºæ–°çš„å…¨å±€ speaker ID
                global_speaker_id = f"SPEAKER_{global_id_counter:02d}"
                cluster_to_global_id[cluster_id] = global_speaker_id
                global_id_counter += 1
            
            id_mapping[speaker_id] = cluster_to_global_id[cluster_id]
        
        logger.info(f"âœ… å…¨å±€æ ¡å‡†å®Œæˆï¼š{len(speaker_ids)} ä¸ªç‰‡æ®µ speaker -> {global_id_counter} ä¸ªå…¨å±€ speaker")
        
        return id_mapping
        
    except Exception as e:
        logger.warning(f"âš ï¸ å…¨å±€æ ¡å‡†å¤±è´¥: {e}")
        return {}


def process_audio_with_pipeline(pipeline, waveform: torch.Tensor, sample_rate: int, 
                                max_chunk_duration: int = 300,
                                use_vad_smart_chunking: bool = True):
    """
    ä½¿ç”¨ Pyannote pipeline å¤„ç†éŸ³é¢‘ï¼ˆæ”¯æŒé•¿éŸ³é¢‘åˆ†æ®µå¤„ç†ï¼‰
    
    Args:
        pipeline: Pyannote pipeline å¯¹è±¡ï¼ˆåº”è¯¥å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼‰
        waveform: éŸ³é¢‘æ³¢å½¢ tensorï¼Œå½¢çŠ¶ä¸º [channels, time]ï¼ˆåº”è¯¥å·²ç»åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼‰
        sample_rate: é‡‡æ ·ç‡
        max_chunk_duration: æœ€å¤§ç‰‡æ®µæ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œè¶…è¿‡æ­¤é•¿åº¦ä¼šåˆ†æ®µå¤„ç†
    
    Returns:
        diarization ç»“æœï¼ˆAnnotation æˆ– DiarizeOutputï¼‰
    """
    audio_duration = waveform.shape[-1] / sample_rate
    
    # æ£€æŸ¥å¹¶ç¡®è®¤ GPU ä½¿ç”¨æƒ…å†µï¼ˆä» pipeline æˆ– waveform è·å–è®¾å¤‡ï¼‰
    if hasattr(pipeline, "device"):
        device = pipeline.device
    elif hasattr(pipeline, "_device"):
        device = pipeline._device
    else:
        # ä» waveform æ¨æ–­è®¾å¤‡ï¼Œæˆ–ä½¿ç”¨é»˜è®¤
        device = waveform.device if hasattr(waveform, "device") else torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    device_info = "GPU" if device.type == "cuda" else "CPU"
    
    if device.type == "cuda" and torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
        gpu_memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
        logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device_info} ({gpu_name})")
        logger.info(f"ğŸ’¾ GPU æ˜¾å­˜: å·²åˆ†é… {gpu_memory_allocated:.2f}GB / å·²ä¿ç•™ {gpu_memory_reserved:.2f}GB")
    else:
        logger.info(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device_info} (CUDA ä¸å¯ç”¨æˆ–æœªä½¿ç”¨)")
    
    # ç¡®ä¿ waveform åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
    if waveform.device != device:
        logger.info(f"ğŸ”„ å°† waveform ä» {waveform.device} ç§»åŠ¨åˆ° {device}")
        waveform = waveform.to(device)
    
    # çŸ­éŸ³é¢‘ç›´æ¥å¤„ç†ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„ pipelineï¼‰
    if audio_duration <= max_chunk_duration:
        logger.info(f"â±ï¸ éŸ³é¢‘æ—¶é•¿ {audio_duration:.1f}ç§’ï¼Œç›´æ¥å¤„ç†")
        return pipeline({"waveform": waveform, "sample_rate": sample_rate})
    
    # é•¿éŸ³é¢‘ï¼šä¼˜åŒ–ç­–ç•¥
    # å¯¹äºè¶…é•¿éŸ³é¢‘ï¼ˆ>1å°æ—¶ï¼‰ï¼Œè·³è¿‡ VAD åˆ‡åˆ†ï¼Œç›´æ¥å›ºå®šåˆ‡åˆ† + å…¨å±€æ ¡å‡†
    # VAD åˆ‡åˆ†è€—æ—¶ä¸”å¯èƒ½äº§ç”Ÿè¿‡å¤šç‰‡æ®µï¼Œå›ºå®šåˆ‡åˆ†æ›´é«˜æ•ˆ
    vad_chunking_success = False
    chunk_boundaries = None
    
    if use_vad_smart_chunking and audio_duration < 3600:
        # åªå¯¹ <1å°æ—¶ çš„éŸ³é¢‘ä½¿ç”¨ VAD åˆ‡åˆ†
        logger.info(f"â±ï¸ éŸ³é¢‘æ—¶é•¿ {audio_duration:.1f}ç§’ï¼Œä½¿ç”¨ VAD æ™ºèƒ½åˆ†æ®µ + å…¨å±€å£°çº¹æ ¡å‡†")
        try:
            from pyannote.core import Annotation, Segment
            
            # ç¬¬ä¸€æ­¥ï¼šVAD æ£€æµ‹é•¿é™éŸ³ç‚¹ï¼ˆ>2ç§’ï¼‰
            silence_segments = detect_long_silence_with_vad(waveform, sample_rate, min_silence_duration=2.0)
            
            # ç¬¬äºŒæ­¥ï¼šæ ¹æ®é™éŸ³ç‚¹åˆ‡åˆ†ä¸º 10-20 åˆ†é’Ÿç‰‡æ®µ
            chunk_boundaries = split_audio_by_silence(
                waveform, sample_rate, silence_segments,
                min_chunk_duration=600.0,  # 10 åˆ†é’Ÿ
                max_chunk_duration=1200.0   # 20 åˆ†é’Ÿ
            )
            
            # å¦‚æœåˆ‡åˆ†ç»“æœä¸ç†æƒ³ï¼ˆç‰‡æ®µå¤ªå¤šæˆ–å¤ªå°‘ï¼‰ï¼Œå›é€€åˆ°å›ºå®šåˆ‡åˆ†
            if not chunk_boundaries or len(chunk_boundaries) == 1 or len(chunk_boundaries) > 50:
                logger.info(f"âš ï¸ VAD åˆ‡åˆ†ç»“æœä¸ç†æƒ³ï¼ˆ{len(chunk_boundaries) if chunk_boundaries else 0}ä¸ªç‰‡æ®µï¼‰ï¼Œå›é€€åˆ°å›ºå®šåˆ‡åˆ†")
                use_vad_smart_chunking = False
                chunk_boundaries = None
            else:
                logger.info(f"âœ… VAD åˆ‡åˆ†æˆåŠŸï¼Œå…± {len(chunk_boundaries)} ä¸ªç‰‡æ®µ")
                vad_chunking_success = True
        except Exception as e:
            logger.warning(f"âš ï¸ VAD åˆ‡åˆ†å¤±è´¥: {e}ï¼Œå›é€€åˆ°å›ºå®šåˆ‡åˆ†")
            use_vad_smart_chunking = False
            chunk_boundaries = None
    
    # å¦‚æœ VAD åˆ‡åˆ†æˆåŠŸï¼Œä½¿ç”¨ VAD åˆ‡åˆ†çš„ç»“æœè¿›è¡Œå¤„ç†
    if vad_chunking_success and chunk_boundaries:
        # ä½¿ç”¨ VAD åˆ‡åˆ†çš„ç»“æœè¿›è¡Œå¤„ç†
        try:
            from pyannote.core import Annotation, Segment
            
            # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ®µæ¨ç†ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
            all_chunk_results = []
            global_embeddings = {}
            
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {len(chunk_boundaries)} ä¸ªç‰‡æ®µï¼ˆVAD åˆ‡åˆ†ï¼‰...")
            
            for chunk_idx, (chunk_start_time, chunk_end_time) in enumerate(chunk_boundaries):
                # è®¡ç®—é‡‡æ ·ç‚¹
                start_sample = int(chunk_start_time * sample_rate)
                end_sample = int(chunk_end_time * sample_rate)
                chunk_waveform = waveform[:, start_sample:end_sample].clone()
                
                if chunk_waveform.shape[-1] < sample_rate * 0.5:
                    logger.warning(f"âš ï¸ ç‰‡æ®µ {chunk_idx + 1} å¤ªçŸ­ï¼ˆ{chunk_waveform.shape[-1] / sample_rate:.2f}ç§’ï¼‰ï¼Œè·³è¿‡")
                    continue
                
                if chunk_waveform.device != device:
                    chunk_waveform = chunk_waveform.to(device)
                
                if chunk_idx % 5 == 0 and device.type == "cuda":
                    torch.cuda.empty_cache()
                
                chunk_duration = chunk_end_time - chunk_start_time
                logger.info(f"ğŸ”„ å¤„ç†ç‰‡æ®µ {chunk_idx + 1}/{len(chunk_boundaries)} ({chunk_start_time:.1f}s - {chunk_end_time:.1f}s, æ—¶é•¿: {chunk_duration:.1f}s) [è®¾å¤‡: {chunk_waveform.device}]")
                
                chunk_diarization = pipeline({"waveform": chunk_waveform, "sample_rate": sample_rate})
                chunk_annotation = _extract_annotation(chunk_diarization)
                
                chunk_embeddings = {}
                try:
                    chunk_embeddings = extract_speaker_embeddings(
                        pipeline, chunk_waveform, sample_rate, chunk_annotation
                    )
                except Exception as e:
                    logger.debug(f"âš ï¸ ç‰‡æ®µ {chunk_idx + 1} embedding æå–å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")
                
                chunk_result = {
                    "chunk_idx": chunk_idx,
                    "start_time": chunk_start_time,
                    "end_time": chunk_end_time,
                    "annotation": chunk_annotation,
                    "embeddings": chunk_embeddings
                }
                all_chunk_results.append(chunk_result)
                
                for speaker_id, embedding in chunk_embeddings.items():
                    global_key = f"chunk_{chunk_idx}_{speaker_id}"
                    global_embeddings[global_key] = embedding.cpu() if embedding.is_cuda else embedding
                
                del chunk_waveform, chunk_diarization, chunk_annotation
                if device.type == "cuda":
                    torch.cuda.empty_cache()
            
            # ç¬¬å››æ­¥ï¼šå…¨å±€å£°çº¹æ ¡å‡†ï¼ˆå¦‚æœæ²¡æœ‰ embeddingï¼Œç›´æ¥åˆå¹¶ï¼Œä¸ä½¿ç”¨æ ¡å‡†ï¼‰
            if len(all_chunk_results) > 1 and global_embeddings:
                logger.info(f"ğŸ”— å¼€å§‹å…¨å±€å£°çº¹æ ¡å‡†ï¼ˆå…± {len(global_embeddings)} ä¸ª speaker embeddingï¼‰...")
                id_mapping = global_speaker_calibration(all_chunk_results, global_embeddings, threshold=0.7)
                
                all_segments = Annotation()
                for chunk_result in all_chunk_results:
                    chunk_idx = chunk_result["chunk_idx"]
                    chunk_start_time = chunk_result["start_time"]
                    chunk_annotation = chunk_result["annotation"]
                    
                    for turn, _, speaker in chunk_annotation.itertracks(yield_label=True):
                        # ç¡®ä¿ speaker ä¸ä¸º None
                        if speaker is None or speaker == "":
                            speaker = f"SPEAKER_{chunk_idx:02d}"
                        
                        chunk_speaker_key = f"chunk_{chunk_idx}_{speaker}"
                        global_speaker_id = id_mapping.get(chunk_speaker_key, speaker)
                        
                        shifted_segment = Segment(
                            turn.start + chunk_start_time,
                            turn.end + chunk_start_time
                        )
                        all_segments[shifted_segment, global_speaker_id] = global_speaker_id
                
                logger.info(f"âœ… å…¨å±€æ ¡å‡†å®Œæˆï¼Œåˆå¹¶ {len(all_chunk_results)} ä¸ªç‰‡æ®µï¼Œå…± {len(all_segments)} ä¸ªè¯´è¯äººç‰‡æ®µ")
                return all_segments
            else:
                # åªæœ‰ä¸€ä¸ªç‰‡æ®µæˆ–æ²¡æœ‰ embeddingï¼Œç›´æ¥åˆå¹¶ï¼ˆä¸ä½¿ç”¨å…¨å±€æ ¡å‡†ï¼‰
                if not global_embeddings:
                    logger.info(f"âš ï¸ æ²¡æœ‰æå–åˆ° embeddingï¼ˆå…± {len(all_chunk_results)} ä¸ªç‰‡æ®µï¼‰ï¼Œè·³è¿‡å…¨å±€æ ¡å‡†ï¼Œç›´æ¥åˆå¹¶")
                else:
                    logger.info(f"âš ï¸ åªæœ‰ä¸€ä¸ªç‰‡æ®µï¼Œè·³è¿‡å…¨å±€æ ¡å‡†ï¼Œç›´æ¥è¿”å›")
                
                all_segments = Annotation()
                for chunk_result in all_chunk_results:
                    chunk_idx = chunk_result["chunk_idx"]
                    chunk_start_time = chunk_result["start_time"]
                    chunk_annotation = chunk_result["annotation"]
                    
                    for turn, _, speaker in chunk_annotation.itertracks(yield_label=True):
                        # ç¡®ä¿ speaker ä¸ä¸º None
                        if speaker is None or speaker == "":
                            speaker = f"SPEAKER_{chunk_idx:02d}"
                        
                        shifted_segment = Segment(
                            turn.start + chunk_start_time,
                            turn.end + chunk_start_time
                        )
                        all_segments[shifted_segment, speaker] = speaker
                
                logger.info(f"âœ… åˆå¹¶å®Œæˆï¼Œå…± {len(all_segments)} ä¸ªè¯´è¯äººç‰‡æ®µ")
                return all_segments
        except Exception as e:
            logger.warning(f"âš ï¸ VAD åˆ‡åˆ†å¤„ç†å¤±è´¥: {e}ï¼Œå›é€€åˆ°å›ºå®šåˆ†æ®µ")
            use_vad_smart_chunking = False
    
    # å›ºå®šåˆ‡åˆ† + å…¨å±€æ ¡å‡†ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
    if not use_vad_smart_chunking or audio_duration >= 3600:
        logger.info(f"â±ï¸ éŸ³é¢‘æ—¶é•¿ {audio_duration:.1f}ç§’ï¼Œä½¿ç”¨å›ºå®šåˆ‡åˆ† + å…¨å±€å£°çº¹æ ¡å‡†ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰")
        try:
            from pyannote.core import Annotation, Segment
            
            # å‡å°ç‰‡æ®µå¤§å°ä»¥é™ä½å†…å­˜å ç”¨ï¼ˆ10-15åˆ†é’Ÿï¼Œé¿å…å†…å­˜æº¢å‡ºï¼‰
            optimized_chunk_duration = min(max_chunk_duration, 900.0)  # æœ€å¤§ 15 åˆ†é’Ÿï¼ˆé™ä½å†…å­˜å ç”¨ï¼‰
            num_chunks = int(audio_duration / optimized_chunk_duration) + 1
            chunk_boundaries = [
                (i * optimized_chunk_duration, min((i + 1) * optimized_chunk_duration, audio_duration))
                for i in range(num_chunks)
            ]
            logger.info(f"âœ‚ï¸ å›ºå®šåˆ‡åˆ†ä¸º {len(chunk_boundaries)} ä¸ªç‰‡æ®µï¼ˆæ¯æ®µçº¦ {optimized_chunk_duration:.0f}ç§’ï¼Œé™ä½å†…å­˜å ç”¨ï¼‰")
            
            # ç¬¬ä¸‰æ­¥ï¼šåˆ†æ®µæ¨ç†ï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰
            all_chunk_results = []
            global_embeddings = {}
            
            logger.info(f"ğŸš€ å¼€å§‹å¤„ç† {len(chunk_boundaries)} ä¸ªç‰‡æ®µï¼ˆä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼‰...")
            
            for chunk_idx, (chunk_start_time, chunk_end_time) in enumerate(chunk_boundaries):
                # è®¡ç®—é‡‡æ ·ç‚¹
                start_sample = int(chunk_start_time * sample_rate)
                end_sample = int(chunk_end_time * sample_rate)
                chunk_waveform = waveform[:, start_sample:end_sample].clone()  # ä½¿ç”¨ clone é¿å…å†…å­˜å…±äº«
                
                if chunk_waveform.shape[-1] < sample_rate * 0.5:  # å°äº0.5ç§’è·³è¿‡
                    continue
                
                # ç¡®ä¿ chunk_waveform åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
                if chunk_waveform.device != device:
                    chunk_waveform = chunk_waveform.to(device)
                
                # æ¸…ç† GPU ç¼“å­˜ï¼ˆå®šæœŸæ¸…ç†ï¼Œé¿å…å†…å­˜ç´¯ç§¯ï¼‰
                if chunk_idx % 5 == 0 and device.type == "cuda":
                    torch.cuda.empty_cache()
                
                chunk_duration = chunk_end_time - chunk_start_time
                logger.info(f"ğŸ”„ å¤„ç†ç‰‡æ®µ {chunk_idx + 1}/{len(chunk_boundaries)} ({chunk_start_time:.1f}s - {chunk_end_time:.1f}s, æ—¶é•¿: {chunk_duration:.1f}s) [è®¾å¤‡: {chunk_waveform.device}]")
                
                # å¤„ç†å½“å‰ç‰‡æ®µ
                chunk_diarization = pipeline({"waveform": chunk_waveform, "sample_rate": sample_rate})
                chunk_annotation = _extract_annotation(chunk_diarization)
                
                # æå–è¯¥ç‰‡æ®µçš„ speaker embeddingsï¼ˆå¦‚æœå¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼‰
                chunk_embeddings = {}
                try:
                    chunk_embeddings = extract_speaker_embeddings(
                        pipeline, chunk_waveform, sample_rate, chunk_annotation
                    )
                except Exception as e:
                    logger.debug(f"âš ï¸ ç‰‡æ®µ {chunk_idx + 1} embedding æå–å¤±è´¥ï¼ˆä¸å½±å“ä¸»æµç¨‹ï¼‰: {e}")
                
                # å­˜å‚¨ç»“æœï¼ˆä½¿ç”¨ç‰‡æ®µå‰ç¼€é¿å… ID å†²çªï¼‰
                chunk_result = {
                    "chunk_idx": chunk_idx,
                    "start_time": chunk_start_time,
                    "end_time": chunk_end_time,
                    "annotation": chunk_annotation,
                    "embeddings": chunk_embeddings
                }
                all_chunk_results.append(chunk_result)
                
                # æ·»åŠ åˆ°å…¨å±€ embedding å­—å…¸ï¼ˆä½¿ç”¨å”¯ä¸€ keyï¼‰
                # å°† embedding ç§»åˆ° CPU ä»¥èŠ‚çœ GPU å†…å­˜
                for speaker_id, embedding in chunk_embeddings.items():
                    global_key = f"chunk_{chunk_idx}_{speaker_id}"
                    global_embeddings[global_key] = embedding.cpu() if embedding.is_cuda else embedding
                
                # æ¸…ç†å½“å‰ç‰‡æ®µçš„ GPU å†…å­˜
                del chunk_waveform, chunk_diarization, chunk_annotation
                if device.type == "cuda":
                    torch.cuda.empty_cache()
            
            # ç¬¬å››æ­¥ï¼šå…¨å±€å£°çº¹æ ¡å‡†
            if len(all_chunk_results) > 1 and global_embeddings:
                logger.info("ğŸ”— å¼€å§‹å…¨å±€å£°çº¹æ ¡å‡†...")
                id_mapping = global_speaker_calibration(all_chunk_results, global_embeddings, threshold=0.7)
                
                # åˆå¹¶æ‰€æœ‰ç‰‡æ®µï¼Œåº”ç”¨æ ¡å‡†åçš„ speaker ID
                all_segments = Annotation()
                for chunk_result in all_chunk_results:
                    chunk_idx = chunk_result["chunk_idx"]
                    chunk_start_time = chunk_result["start_time"]
                    chunk_annotation = chunk_result["annotation"]
                    
                    for turn, _, speaker in chunk_annotation.itertracks(yield_label=True):
                        # æŸ¥æ‰¾æ ¡å‡†åçš„å…¨å±€ speaker ID
                        chunk_speaker_key = f"chunk_{chunk_idx}_{speaker}"
                        global_speaker_id = id_mapping.get(chunk_speaker_key, speaker)
                        
                        # åˆ›å»ºæ–°çš„ Segmentï¼Œæ—¶é—´åŠ ä¸Šåç§»é‡
                        shifted_segment = Segment(
                            turn.start + chunk_start_time,
                            turn.end + chunk_start_time
                        )
                        all_segments[shifted_segment, global_speaker_id] = global_speaker_id
                
                logger.info(f"âœ… å…¨å±€æ ¡å‡†å®Œæˆï¼Œåˆå¹¶ {len(all_chunk_results)} ä¸ªç‰‡æ®µ")
                return all_segments
            else:
                # åªæœ‰ä¸€ä¸ªç‰‡æ®µæˆ–æ²¡æœ‰ embeddingï¼Œç›´æ¥åˆå¹¶
                all_segments = Annotation()
                for chunk_result in all_chunk_results:
                    chunk_start_time = chunk_result["start_time"]
                    chunk_annotation = chunk_result["annotation"]
                    
                    for turn, _, speaker in chunk_annotation.itertracks(yield_label=True):
                        shifted_segment = Segment(
                            turn.start + chunk_start_time,
                            turn.end + chunk_start_time
                        )
                        all_segments[shifted_segment, speaker] = speaker
                
                return all_segments
                
        except Exception as e:
            logger.warning(f"âš ï¸ VAD æ™ºèƒ½åˆ†æ®µå¤±è´¥ï¼Œå›é€€åˆ°å›ºå®šåˆ†æ®µ: {e}")
            # å›é€€åˆ°åŸæ¥çš„å›ºå®šåˆ†æ®µé€»è¾‘
            use_vad_smart_chunking = False
    
    # å›é€€ï¼šå›ºå®šæ—¶é•¿åˆ†æ®µï¼ˆåŸé€»è¾‘ï¼‰
    logger.info(f"â±ï¸ éŸ³é¢‘æ—¶é•¿ {audio_duration:.1f}ç§’ï¼Œä½¿ç”¨å›ºå®šåˆ†æ®µå¤„ç†ï¼ˆæ¯æ®µ {max_chunk_duration}ç§’ï¼‰")
    try:
        from pyannote.core import Annotation, Segment
        
        all_segments = Annotation()
        num_chunks = int(audio_duration / max_chunk_duration) + 1
        
        for chunk_idx in range(num_chunks):
            chunk_start_time = chunk_idx * max_chunk_duration
            chunk_end_time = min((chunk_idx + 1) * max_chunk_duration, audio_duration)
            
            start_sample = int(chunk_start_time * sample_rate)
            end_sample = int(chunk_end_time * sample_rate)
            chunk_waveform = waveform[:, start_sample:end_sample]
            
            if chunk_waveform.shape[-1] < sample_rate * 0.5:
                continue
            
            if chunk_waveform.device != device:
                chunk_waveform = chunk_waveform.to(device)
            
            logger.info(f"ğŸ”„ å¤„ç†ç‰‡æ®µ {chunk_idx + 1}/{num_chunks} ({chunk_start_time:.1f}s - {chunk_end_time:.1f}s) [è®¾å¤‡: {chunk_waveform.device}]")
            
            chunk_diarization = pipeline({"waveform": chunk_waveform, "sample_rate": sample_rate})
            chunk_annotation = _extract_annotation(chunk_diarization)
            
            for turn, _, speaker in chunk_annotation.itertracks(yield_label=True):
                # ç¡®ä¿ speaker ä¸ä¸º None æˆ–ç©º
                if speaker is None or speaker == "":
                    speaker = f"SPEAKER_{chunk_idx:02d}"  # ä½¿ç”¨ç‰‡æ®µç´¢å¼•ä½œä¸ºé»˜è®¤ speaker
                
                shifted_segment = Segment(turn.start + chunk_start_time, turn.end + chunk_start_time)
                all_segments[shifted_segment, speaker] = speaker
        
        return all_segments
    except ImportError:
        logger.warning("âš ï¸ æ— æ³•å¯¼å…¥ pyannote.coreï¼Œé™çº§ä¸ºå…¨é‡å¤„ç†")
        return pipeline({"waveform": waveform, "sample_rate": sample_rate})


def get_pyannote_pipeline(use_auth_token: Optional[str] = None):
    """
    è·å– Pyannote pipelineï¼ˆä¼˜å…ˆä½¿ç”¨é¡¹ç›®æœ¬åœ° models/ ç›®å½•ï¼Œå¼ºåˆ¶ç¦»çº¿ï¼Œé¿å…ä»»ä½•è”ç½‘ï¼‰
    """
    global _pipeline_cache
    
    if _pipeline_cache is not None:
        return _pipeline_cache
    
    if not PYANNOTE_AVAILABLE:
        return None
    
    # ç»Ÿä¸€ tokenï¼ˆå…¼å®¹å†å²å‘½å use_auth_tokenï¼‰
    hf_token = use_auth_token or os.getenv("HF_TOKEN") or os.getenv("HUGGINGFACE_TOKEN") or None

    # å¼ºåˆ¶ç¦»çº¿ï¼šå½»åº•ç¦æ­¢ huggingface_hub å‘èµ·ä»»ä½•è¯·æ±‚
    # è¯´æ˜ï¼šå¦‚æœæœ¬åœ°æ¨¡å‹ä¸å®Œæ•´ï¼Œä¼šç›´æ¥æŠ¥ç¼ºæ–‡ä»¶ï¼Œè€Œä¸æ˜¯å·å·è”ç½‘è¡¥é½
    # ç”¨å¼ºåˆ¶è¦†ç›–è€Œä¸æ˜¯ setdefaultï¼Œé¿å…å¤–éƒ¨ç¯å¢ƒæå‰è®¾ç½®ä¸º 0 å¯¼è‡´å¤±æ•ˆ
    os.environ["HF_HUB_OFFLINE"] = "1"
    os.environ["TRANSFORMERS_OFFLINE"] = "1"

    def _safe_from_pretrained(model_ref: str):
        """å…¼å®¹ä¸åŒç‰ˆæœ¬ pyannote.audio / huggingface å‚æ•°åï¼ˆtoken vs use_auth_tokenï¼‰"""
        # æœ¬åœ°è·¯å¾„åŠ è½½ï¼šä¸è¦ä¼  token/use_auth_tokenï¼ˆé¿å…å‚æ•°ä¸å…¼å®¹ï¼Œä¹Ÿé¿å…è§¦å‘ hub é€»è¾‘ï¼‰
        if Path(model_ref).exists():
            # è¿™é‡Œå¦‚æœå¤±è´¥ï¼Œåº”è¯¥ç›´æ¥æŠ¥é”™ï¼ˆä¸è¦å†å›é€€åˆ° token/use_auth_tokenï¼Œå¦åˆ™ä¼šè¯¯è§¦å‘ hub é€»è¾‘ï¼‰
            return Pipeline.from_pretrained(model_ref)

        # è¿œç«¯ï¼ˆæˆ– cacheï¼‰åŠ è½½ï¼šæ ¹æ®å‚æ•°ååšå…¼å®¹
        try:
            if hf_token:
                return Pipeline.from_pretrained(model_ref, token=hf_token)
            return Pipeline.from_pretrained(model_ref)
        except TypeError as e:
            # åªæœ‰å½“ç¡®å®æ˜¯â€œtoken å‚æ•°ä¸è¢«æ”¯æŒâ€æ—¶ï¼Œæ‰å›é€€ use_auth_token
            msg = str(e)
            if hf_token and ("token" in msg or "unexpected keyword argument" in msg):
                return Pipeline.from_pretrained(model_ref, use_auth_token=hf_token)
            raise

    try:
        # 1) ä¼˜å…ˆä»é¡¹ç›®æœ¬åœ° models/ ç›®å½•åŠ è½½ï¼ˆæ¨èã€ç¦»çº¿å¯ç”¨ï¼‰
        project_root = Path(__file__).resolve().parent.parent
        local_diar_dir = project_root / "models" / "pyannote_diarization"
        local_cfg = local_diar_dir / "config.yaml"
        local_offline_cfg = local_diar_dir / "offline_config.yaml"
        local_seg_dir = project_root / "models" / "pyannote_segmentation"
        local_emb_dir = project_root / "models" / "pyannote_wespeaker"

        if local_diar_dir.exists() and local_cfg.exists():
            cfg_source = local_offline_cfg if local_offline_cfg.exists() else local_cfg
            logger.info(f"âœ… ä½¿ç”¨æœ¬åœ° Pyannote é…ç½®åŠ è½½ï¼ˆç¦»çº¿ï¼‰: {cfg_source}")

            # æ£€æŸ¥å­æ¨¡å‹ç›®å½•æ˜¯å¦å­˜åœ¨
            if not local_seg_dir.exists():
                logger.error(f"âŒ ç¼ºå°‘æœ¬åœ° segmentation æ¨¡å‹ç›®å½•: {local_seg_dir}")
                return None
            if not local_emb_dir.exists():
                logger.error(f"âŒ ç¼ºå°‘æœ¬åœ° embedding æ¨¡å‹ç›®å½•: {local_emb_dir}")
                return None

            # åŠ¨æ€æ”¹å†™ config.yamlï¼šæŠŠ segmentation/embedding æŒ‡åˆ°æœ¬åœ°ç›®å½•ï¼ˆè€Œä¸æ˜¯ HuggingFace ID æˆ–å…·ä½“ bin æ–‡ä»¶ï¼‰
            try:
                import yaml  # PyYAML
            except Exception as e:
                logger.error(f"âŒ ç¼ºå°‘ PyYAMLï¼Œæ— æ³•æ”¹å†™ config.yaml: {e}")
                logger.error("   è¯·å®‰è£…: pip install PyYAML")
                return None

            original_cfg_bytes = local_cfg.read_bytes()
            try:
                cfg_obj = yaml.safe_load(cfg_source.read_text(encoding="utf-8"))
                if not isinstance(cfg_obj, dict):
                    raise ValueError("config.yaml è§£æç»“æœä¸æ˜¯ dict")

                pipeline_section = cfg_obj.setdefault("pipeline", {})
                params = pipeline_section.setdefault("params", {})

                # æ—§ç‰ˆ offline_config é‡Œå¯èƒ½æŠŠ clustering é…æˆ dictï¼Œä½†ä½ è¿™ç‰ˆ pyannote.audio æœŸæœ›æ˜¯å­—ç¬¦ä¸² keyã€‚
                # è¿™é‡Œç›´æ¥åˆ æ‰ clusteringï¼Œè®© pipeline ä½¿ç”¨è‡ªå·±çš„é»˜è®¤èšç±»é…ç½®ï¼ˆæˆ‘ä»¬åªæ§åˆ¶æ¨¡å‹è·¯å¾„å’Œ PLDAï¼‰ã€‚
                # æ³¨æ„ï¼šclustering å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "centroid"ï¼‰ï¼Œä¸èƒ½æ˜¯ dict
                if isinstance(params.get("clustering"), dict):
                    logger.info("â„¹ï¸ æ£€æµ‹åˆ° dict ç±»å‹ clustering é…ç½®ï¼Œå·²ç§»é™¤ä»¥ä½¿ç”¨é»˜è®¤èšç±»ç­–ç•¥")
                    params.pop("clustering", None)
                # ç¡®ä¿ clustering ä¸æ˜¯ dictï¼ˆå¦‚æœå­˜åœ¨ï¼Œå¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼‰
                if "clustering" in params and isinstance(params["clustering"], dict):
                    params.pop("clustering", None)
                    logger.info("â„¹ï¸ å·²ç§»é™¤ dict ç±»å‹ clustering é…ç½®")

                # å…³é”®ï¼šæ˜¾å¼æŒ‡å®š PLDA èµ„æºæ¥æºã€‚
                # è¯´æ˜ï¼šå½“å‰ pyannote.audio ç‰ˆæœ¬ä¸æ¥å— plda=Noneï¼ˆåªæ¥å— str/dictï¼‰ï¼Œ
                # ä¸” SpeakerDiarization é»˜è®¤ä¼šåŠ è½½ `pyannote/speaker-diarization-community-1`
                # å¹¶åœ¨ç¼º cache æ—¶å°è¯•è”ç½‘ä¸‹è½½ plda/xvec_transform.npzã€‚
                # æˆ‘ä»¬åœ¨å¼ºåˆ¶ç¦»çº¿æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨é¡¹ç›®æœ¬åœ°ç›®å½•ï¼Œå¦åˆ™é€€å›åˆ° HF cacheï¼ˆä¸ä¼šè”ç½‘ï¼‰ã€‚
                local_plda_candidates = [
                    project_root / "models" / "pyannote_speaker_diarization_community_1",
                    project_root / "models" / "speaker-diarization-community-1",
                    project_root / "models" / "pyannote_plda",
                ]
                plda_ref = None
                for cand in local_plda_candidates:
                    # community-1 å¯èƒ½æ˜¯ `xvec_transform.npz` æ”¾åœ¨æ ¹ç›®å½•ï¼Œä¹Ÿå¯èƒ½åœ¨ `plda/` å­ç›®å½•
                    xvec_root = cand / "xvec_transform.npz"
                    plda_root = cand / "plda.npz"
                    xvec_sub = cand / "plda" / "xvec_transform.npz"
                    plda_sub = cand / "plda" / "plda.npz"

                    if xvec_root.exists() and plda_root.exists():
                        # ç›´æ¥ä½¿ç”¨ç›®å½•è·¯å¾„ï¼ˆpyannote ä¼šåœ¨ç›®å½•æ ¹ä¸‹æ‰¾è¿™ä¸¤ä¸ª npzï¼‰
                        plda_ref = str(cand.resolve())
                        break
                    if xvec_sub.exists() and plda_sub.exists():
                        # å…¼å®¹ï¼šæœ‰çš„ repo æŠŠ npz æ”¾åœ¨ plda/ å­ç›®å½•é‡Œï¼Œä½†éƒ¨åˆ† pyannote ç‰ˆæœ¬åªä¼šåœ¨æ ¹ç›®å½•æ‰¾ã€‚
                        # è¿™é‡Œå°†ä¸¤ä¸ª npz â€œå±•å¼€/å¤åˆ¶â€ åˆ°ä¸€ä¸ªæ‰å¹³ç›®å½•ä¸­ï¼Œå†ç”¨å­—ç¬¦ä¸²è·¯å¾„æŒ‡å‘è¯¥ç›®å½•ã€‚
                        flat_dir = project_root / "models" / "_pyannote_plda_flat"
                        flat_dir.mkdir(parents=True, exist_ok=True)
                        flat_xvec = flat_dir / "xvec_transform.npz"
                        flat_plda = flat_dir / "plda.npz"
                        try:
                            shutil.copy2(xvec_sub, flat_xvec)
                            shutil.copy2(plda_sub, flat_plda)
                            logger.info(f"âœ… å·²å±•å¼€ PLDA æ–‡ä»¶åˆ°æ‰å¹³ç›®å½•: {flat_dir}")
                        except Exception as e:
                            logger.error(f"âŒ å±•å¼€ PLDA æ–‡ä»¶å¤±è´¥: {e}")
                            return None
                        plda_ref = str(flat_dir.resolve())
                        break
                if plda_ref:
                    params["plda"] = plda_ref
                    logger.info(f"âœ… ä½¿ç”¨æœ¬åœ° PLDA èµ„æº: {plda_ref}")
                else:
                    # ä½¿ç”¨ HF cacheï¼ˆç¦»çº¿ç¯å¢ƒä¸‹ä¸ä¼šä¸‹è½½ï¼›è‹¥ cache ä¸å­˜åœ¨ï¼Œä¼šæ˜ç¡®æŠ¥ç¼ºæ–‡ä»¶ï¼‰
                    params["plda"] = "pyannote/speaker-diarization-community-1"
                    logger.warning("âš ï¸ æœªæ‰¾åˆ°é¡¹ç›®æœ¬åœ° PLDA èµ„æºç›®å½•ï¼Œå°†ä½¿ç”¨ HF cache: pyannote/speaker-diarization-community-1ï¼ˆå·²å¼ºåˆ¶ç¦»çº¿ï¼Œä¸ä¼šè”ç½‘ï¼‰")

                # å…³é”®ï¼šæŒ‡å‘æœ¬åœ°ç›®å½•ï¼ˆç›®å½•é‡ŒåŒ…å« config.yaml / pytorch_model.bin ç­‰ï¼‰
                params["segmentation"] = str(local_seg_dir.resolve())
                params["embedding"] = str(local_emb_dir.resolve())
                
                # æ€§èƒ½ä¼˜åŒ–ï¼šåœ¨ config.yaml ä¸­è®¾ç½®å‚æ•°ï¼ˆåœ¨ pipeline åŠ è½½å‰è®¾ç½®ï¼‰
                # æ ¹æ®è­¦å‘Šä¿¡æ¯ï¼Œpipeline ä¼šè‡ªåŠ¨å®ä¾‹åŒ–ï¼Œæˆ‘ä»¬éœ€è¦åœ¨ config ä¸­è®¾ç½®å‚æ•°
                # æ–¹æ³•ï¼šå°† segmentation ä»å­—ç¬¦ä¸²è·¯å¾„æ”¹ä¸ºå­—å…¸é…ç½®ï¼ŒåŒ…å«æ¨¡å‹è·¯å¾„å’Œå‚æ•°
                seg_path = params.get("segmentation", str(local_seg_dir.resolve()))
                # æ³¨æ„ï¼šsegmentation å¿…é¡»ä¿æŒä¸ºå­—ç¬¦ä¸²è·¯å¾„æˆ–åŒ…å« checkpoint çš„å­—å…¸
                # Pyannote çš„ get_model å‡½æ•°æœŸæœ› checkpoint å‚æ•°ï¼Œä¸æ˜¯ model
                if isinstance(seg_path, str):
                    # ä¿æŒå­—ç¬¦ä¸²è·¯å¾„ï¼Œmin_duration å‚æ•°å°†åœ¨ pipeline å®ä¾‹åŒ–åè®¾ç½®
                    params["segmentation"] = seg_path
                    logger.info("âš™ï¸ segmentation ä¿æŒä¸ºè·¯å¾„æ ¼å¼ï¼Œmin_duration å‚æ•°å°†åœ¨ pipeline å®ä¾‹åŒ–åè®¾ç½®")
                elif isinstance(seg_path, dict):
                    # å¦‚æœå·²ç»æ˜¯å­—å…¸ï¼Œç¡®ä¿ä½¿ç”¨ checkpoint è€Œä¸æ˜¯ model
                    if "model" in seg_path and "checkpoint" not in seg_path:
                        seg_path["checkpoint"] = seg_path.pop("model")
                        logger.info("âš™ï¸ å·²å°† segmentation.model æ”¹ä¸º checkpoint")
                    # æ·»åŠ æ€§èƒ½ä¼˜åŒ–å‚æ•°
                    if "checkpoint" in seg_path:
                        seg_path.setdefault("min_duration_on", 0.5)
                        seg_path.setdefault("min_duration_off", 0.5)
                        logger.info("âš™ï¸ å·²æ›´æ–° segmentation å‚æ•°: min_duration_on=0.5, min_duration_off=0.5")
                    else:
                        logger.warning("âš ï¸ segmentation å­—å…¸ä¸­ç¼ºå°‘ checkpoint å‚æ•°ï¼Œå°†ä½¿ç”¨å­—ç¬¦ä¸²è·¯å¾„")
                        params["segmentation"] = str(local_seg_dir.resolve())
                
                # æ³¨æ„ï¼šclustering å¿…é¡»æ˜¯å­—ç¬¦ä¸²ï¼ˆå¦‚ "centroid"ï¼‰ï¼Œä¸èƒ½æ˜¯ dict
                # å¦‚æœéœ€è¦ä¼˜åŒ–èšç±»ï¼Œåº”è¯¥åœ¨ pipeline å®ä¾‹åŒ–åé€šè¿‡å…¶ä»–æ–¹å¼è®¾ç½®
                # è¿™é‡Œä¸è®¾ç½® clustering å‚æ•°ï¼Œè®© pipeline ä½¿ç”¨é»˜è®¤å€¼

                # å†™å…¥åˆ° models/pyannote_diarization/config.yamlï¼ˆä¸´æ—¶è¦†ç›–ï¼ŒåŠ è½½åæ¢å¤ï¼‰
                local_cfg.write_text(yaml.safe_dump(cfg_obj, sort_keys=False, allow_unicode=True), encoding="utf-8")
                logger.info("âœ… å·²å°† segmentation/embedding æ˜ å°„åˆ°æœ¬åœ° models ç›®å½•ï¼Œå¹¶è®¾ç½®æ€§èƒ½ä¼˜åŒ–å‚æ•°")

                pipeline = _safe_from_pretrained(str(local_diar_dir))
                
                # å…³é”®ï¼šå°† pipeline ç§»åŠ¨åˆ° GPUï¼ˆå¦‚æœå¯ç”¨ï¼‰
                device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
                if torch.cuda.is_available():
                    pipeline = pipeline.to(device)
                    logger.info(f"âœ… Pipeline å·²ç§»åŠ¨åˆ° GPU: {device}")
                    # æ˜¾ç¤º GPU ä¿¡æ¯
                    gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "N/A"
                    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3 if torch.cuda.is_available() else 0
                    logger.info(f"ğŸ® GPU ä¿¡æ¯: {gpu_name}, æ˜¾å­˜: {gpu_memory:.1f}GB")
                else:
                    logger.warning("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPUï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰")
                
                # æ€§èƒ½ä¼˜åŒ–ï¼šåœ¨ pipeline å®ä¾‹åŒ–åè®¾ç½®å‚æ•°
                # æ ¹æ®è­¦å‘Šä¿¡æ¯ï¼Œpipeline ä¼šè‡ªåŠ¨å®ä¾‹åŒ–ï¼Œæˆ‘ä»¬éœ€è¦åœ¨å®ä¾‹åŒ–åè®¾ç½®å‚æ•°
                try:
                    # æ–¹æ³•1ï¼šå°è¯•é€šè¿‡ pipeline çš„å†…éƒ¨å±æ€§è®¾ç½®å‚æ•°
                    # Pyannote 3.x çš„ pipeline å¯èƒ½æœ‰ä¸åŒçš„å†…éƒ¨ç»“æ„
                    if hasattr(pipeline, "segmentation"):
                        seg = pipeline.segmentation
                        
                        # å°è¯•å¤šç§æ–¹å¼è®¾ç½®å‚æ•°
                        attrs_to_try = [
                            ("min_duration_on", 0.5),
                            ("min_duration_off", 0.5),
                        ]
                        
                        for attr_name, attr_value in attrs_to_try:
                            if hasattr(seg, attr_name):
                                setattr(seg, attr_name, attr_value)
                                logger.info(f"âš™ï¸ å·²è®¾ç½® segmentation.{attr_name}: {attr_value}ç§’")
                            # å°è¯•é€šè¿‡å†…éƒ¨å¯¹è±¡è®¾ç½®
                            elif hasattr(seg, "_segmentation"):
                                inner_seg = seg._segmentation
                                if hasattr(inner_seg, attr_name):
                                    setattr(inner_seg, attr_name, attr_value)
                                    logger.info(f"âš™ï¸ å·²è®¾ç½® segmentation._segmentation.{attr_name}: {attr_value}ç§’")
                            # å°è¯•é€šè¿‡ params è®¾ç½®
                            elif hasattr(seg, "params") and isinstance(seg.params, dict):
                                seg.params[attr_name] = attr_value
                                logger.info(f"âš™ï¸ å·²é€šè¿‡ segmentation.params è®¾ç½® {attr_name}: {attr_value}ç§’")
                    
                    # æ–¹æ³•2ï¼šå°è¯•è®¾ç½®æ‰¹å¤„ç†å¤§å°å’Œä¼˜åŒ–èšç±»å‚æ•°
                    if hasattr(pipeline, "batch_size"):
                        pipeline.batch_size = 32
                        logger.info(f"âš™ï¸ å·²è®¾ç½® pipeline.batch_size: 32")
                    elif hasattr(pipeline, "segmentation") and hasattr(pipeline.segmentation, "batch_size"):
                        pipeline.segmentation.batch_size = 32
                        logger.info(f"âš™ï¸ å·²è®¾ç½® segmentation.batch_size: 32")
                    
                    # æ–¹æ³•3ï¼šä¼˜åŒ–èšç±»å‚æ•°ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
                    if hasattr(pipeline, "clustering"):
                        clustering = pipeline.clustering
                        if hasattr(clustering, "threshold"):
                            clustering.threshold = 0.7  # æé«˜é˜ˆå€¼ï¼Œå‡å°‘ç‰‡æ®µ
                            logger.info("âš™ï¸ å·²è®¾ç½® clustering.threshold: 0.7ï¼ˆå‡å°‘ç‰‡æ®µæ•°é‡ï¼‰")
                except Exception as e:
                    logger.debug(f"âš ï¸ è®¾ç½® pipeline å‚æ•°å¤±è´¥ï¼ˆå°†ä½¿ç”¨è°ƒç”¨æ—¶ä¼ é€’å‚æ•°ï¼‰: {e}")
                
                _pipeline_cache = pipeline
                logger.info("âœ… æœ¬åœ° Pyannote pipeline åŠ è½½æˆåŠŸï¼ˆå…¨ç¨‹ç¦»çº¿ï¼Œå·²ä¼˜åŒ–æ€§èƒ½å‚æ•°ï¼‰")
                return pipeline
            except Exception as e:
                logger.error(f"âŒ æœ¬åœ° Pyannote pipeline åŠ è½½å¤±è´¥: {e}", exc_info=True)
                return None
            finally:
                # æ¢å¤åŸ config.yamlï¼Œé¿å…æ±¡æŸ“ä»“åº“æ–‡ä»¶
                try:
                    local_cfg.write_bytes(original_cfg_bytes)
                except Exception:
                    pass

        # 2) å¦‚æœæ²¡æ‰¾åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•ï¼šæ˜ç¡®æç¤ºï¼ˆä¸å†å°è¯•è”ç½‘ï¼‰
        logger.error("âŒ æœªæ‰¾åˆ°æœ¬åœ° Pyannote æ¨¡å‹ç›®å½•æˆ–é…ç½®æ–‡ä»¶ï¼ˆmodels/pyannote_diarization/config.yamlï¼‰")
        logger.error(f"   æœŸæœ›è·¯å¾„: {local_diar_dir}")
        return None

    except Exception as e:
        logger.error(f"âŒ Pyannote åˆå§‹åŒ–é”™è¯¯: {e}", exc_info=True)
        return None
    
    return None


def perform_pyannote_diarization(
    audio_path: str,
    transcript: List[Dict],
    use_auth_token: Optional[str] = None
) -> List[Dict]:
    """
    ä½¿ç”¨ Pyannote è¿›è¡Œè¯´è¯äººåˆ†ç¦»
    """
    if not PYANNOTE_AVAILABLE:
        logger.error("âŒ Pyannote.audio æœªå®‰è£…")
        return transcript

    try:
        logger.info("ğŸ¤ ä½¿ç”¨ Pyannote.audio è¿›è¡Œè¯´è¯äººåˆ†ç¦»...")
        
        # 1. è·å– pipeline (é€»è¾‘éƒ½åœ¨ get_pyannote_pipeline é‡Œå¤„ç†å¥½äº†)
        pipeline = get_pyannote_pipeline(use_auth_token)
        
        if pipeline is None:
            logger.error("âŒ æ— æ³•åŠ è½½ Pyannote pipelineï¼Œè·³è¿‡åˆ†ç¦»æ­¥éª¤")
            # é™çº§å¤„ç†ï¼šå…¨æ ‡è®°ä¸º 0
            for item in transcript:
                if 'speaker_id' not in item:
                    item['speaker_id'] = "0"
            return transcript

        # 2. å¤„ç†éŸ³é¢‘ï¼ˆæ”¯æŒ URLï¼šå¦‚æœæ˜¯ http(s)ï¼Œåœ¨æœåŠ¡å™¨ç«¯å…ˆä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶å†æ¨ç†ï¼‰
        logger.info(f"ğŸ“‚ å¤„ç†éŸ³é¢‘æ–‡ä»¶: {audio_path}")
        tmp_path = None
        try:
            if isinstance(audio_path, str) and audio_path.startswith(("http://", "https://")):
                import requests
                import tempfile

                logger.info(f"ğŸ”— æ£€æµ‹åˆ°éŸ³é¢‘ URLï¼Œæ­£åœ¨æœåŠ¡å™¨ç«¯ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶: {audio_path}")
                resp = requests.get(audio_path, timeout=300, stream=True)
                resp.raise_for_status()

                suffix = Path(audio_path).suffix or ".mp3"
                with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
                    for chunk in resp.iter_content(chunk_size=8192):
                        if chunk:
                            tmp.write(chunk)
                    tmp_path = tmp.name
                logger.info(f"âœ… éŸ³é¢‘å·²ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶: {tmp_path}")
                audio_path = tmp_path

            # æ‰‹åŠ¨è§£ç éŸ³é¢‘ï¼Œç»•è¿‡ pyannote å†…éƒ¨ AudioDecoder
            if not os.path.exists(audio_path):
                raise FileNotFoundError(f"Audio file not found: {audio_path}")

            # å¦‚æœæ ¼å¼ä¸æ”¯æŒï¼ˆå¦‚ M4Aï¼‰ï¼Œä½¿ç”¨ ffmpeg è½¬æ¢
            converted_audio_path = None
            try:
                # å°è¯•ç›´æ¥è¯»å–
                data, sample_rate = sf.read(audio_path)
            except Exception as e:
                # å¦‚æœ soundfile ä¸æ”¯æŒè¯¥æ ¼å¼ï¼Œä½¿ç”¨ ffmpeg è½¬æ¢ä¸º WAV
                logger.info(f"âš ï¸ soundfile ä¸æ”¯æŒè¯¥æ ¼å¼ï¼Œä½¿ç”¨ ffmpeg è½¬æ¢: {audio_path}")
                try:
                    # ä½¿ç”¨ ffmpeg è½¬æ¢ä¸º WAV
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_wav:
                        converted_audio_path = tmp_wav.name
                    
                    cmd = [
                        "ffmpeg", "-i", audio_path,
                        "-ac", "1",  # å•å£°é“
                        "-ar", "16000",  # 16kHz é‡‡æ ·ç‡
                        "-f", "wav",
                        "-y",  # è¦†ç›–è¾“å‡ºæ–‡ä»¶
                        converted_audio_path
                    ]
                    
                    result = subprocess.run(
                        cmd,
                        check=True,
                        capture_output=True,
                        timeout=60
                    )
                    
                    # è¯»å–è½¬æ¢åçš„ WAV æ–‡ä»¶
                    data, sample_rate = sf.read(converted_audio_path)
                    logger.info(f"âœ… éŸ³é¢‘æ ¼å¼è½¬æ¢æˆåŠŸ: {converted_audio_path}")
                except subprocess.CalledProcessError as ffmpeg_error:
                    error_msg = ffmpeg_error.stderr.decode() if ffmpeg_error.stderr else str(ffmpeg_error)
                    raise RuntimeError(f"ffmpeg è½¬æ¢å¤±è´¥: {error_msg}") from e
                except FileNotFoundError:
                    raise RuntimeError("ffmpeg æœªå®‰è£…ï¼Œæ— æ³•å¤„ç† M4A ç­‰æ ¼å¼ã€‚è¯·å®‰è£…: apt-get install ffmpeg æˆ– conda install ffmpeg") from e
            if data.ndim == 1:
                data = data[None, :]
            else:
                data = data.T

            waveform = torch.tensor(data, dtype=torch.float32)
            
            # ç¡®ä¿ waveform åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆä¸ pipeline ä¸€è‡´ï¼‰
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            if waveform.device != device:
                waveform = waveform.to(device)
                logger.info(f"ğŸ”„ å·²å°† waveform ç§»åŠ¨åˆ°è®¾å¤‡: {device}")
            
            # ä½¿ç”¨å…¬å…±å‡½æ•°å¤„ç†éŸ³é¢‘ï¼ˆæ”¯æŒé•¿éŸ³é¢‘åˆ†æ®µå¤„ç†ï¼‰
            diarization = process_audio_with_pipeline(pipeline, waveform, sample_rate)
        finally:
            # æ¸…ç† URL ä¸‹è½½äº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶
            if tmp_path and os.path.exists(tmp_path):
                try:
                    os.remove(tmp_path)
                    logger.info(f"ğŸ§¹ å·²æ¸…ç† Pyannote ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶: {tmp_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç† Pyannote ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å¤±è´¥: {tmp_path}, {e}")
            
            # æ¸…ç†æ ¼å¼è½¬æ¢äº§ç”Ÿçš„ä¸´æ—¶æ–‡ä»¶
            if 'converted_audio_path' in locals() and converted_audio_path and os.path.exists(converted_audio_path):
                try:
                    os.remove(converted_audio_path)
                    logger.debug(f"ğŸ§¹ å·²æ¸…ç†æ ¼å¼è½¬æ¢ä¸´æ—¶æ–‡ä»¶: {converted_audio_path}")
                except Exception as e:
                    logger.warning(f"âš ï¸ æ¸…ç†æ ¼å¼è½¬æ¢ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        # 3. å…¼å®¹ä¸åŒç‰ˆæœ¬è¾“å‡ºï¼Œæ‹¿åˆ°çœŸæ­£çš„ Annotation
        annotation = _extract_annotation(diarization)

        # 4. æ„å»ºè¯´è¯äººæ—¶é—´æ˜ å°„
        speaker_segments = []
        for turn, _, speaker in annotation.itertracks(yield_label=True):
            speaker_segments.append({
                'start_time': turn.start,
                'end_time': turn.end,
                'speaker_id': speaker
            })
        
        logger.info(f"âœ… Pyannote è¯†åˆ«å‡º {len(set(s['speaker_id'] for s in speaker_segments))} ä¸ªè¯´è¯äºº")

        # 5. å°†è¯´è¯äººä¿¡æ¯æ˜ å°„åˆ° transcript (åé¢çš„ä»£ç ä¿æŒä¸å˜)
        for item in transcript:
            item_start = item.get('start_time', 0)
            item_end = item.get('end_time', 0)
            
            # ... (è¿™éƒ¨åˆ†çš„å¯¹é½é€»è¾‘ä½ åŸæ¥å†™çš„æ²¡é—®é¢˜ï¼Œä¿ç•™) ...
            matched_speaker = None
            max_overlap = 0
            
            for seg in speaker_segments:
                seg_start = seg['start_time']
                seg_end = seg['end_time']
                overlap_start = max(item_start, seg_start)
                overlap_end = min(item_end, seg_end)
                overlap = max(0, overlap_end - overlap_start)
                
                item_duration = item_end - item_start
                if item_duration > 0 and overlap / item_duration > 0.5:
                    if overlap > max_overlap:
                        max_overlap = overlap
                        matched_speaker = seg['speaker_id']
            
            item['speaker_id'] = matched_speaker if matched_speaker else "SPEAKER_00"

        # 5. è§„èŒƒåŒ– ID
        speaker_id_map = {}
        speaker_counter = 0
        for item in transcript:
            original_id = item.get('speaker_id', 'SPEAKER_00')
            if original_id not in speaker_id_map:
                speaker_id_map[original_id] = str(speaker_counter)
                speaker_counter += 1
            item['speaker_id'] = speaker_id_map[original_id]
        
        return transcript
        
    except Exception as e:
        logger.error(f"âŒ Pyannote è¯´è¯äººåˆ†ç¦»å¤±è´¥: {e}", exc_info=True)
        for item in transcript:
            item['speaker_id'] = "0"
        return transcript
